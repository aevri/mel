#! /usr/bin/env python
"""Train 'automark4'."""

import argparse
import json
import sys

from pprint import pprint

import pytorch_lightning as pl
import wandb


import mel.rotomap.moles
import mel.lib.common
import mel.rotomap.detectmolesnn


def setup_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train",
        metavar="PTGZ_FILE",
        nargs="+",
        help="A list of paths to pre-trained images to process.",
        required=True,
    )
    parser.add_argument(
        "--valid",
        metavar="PTGZ_FILE",
        nargs="*",
        help="A list of paths to pre-trained images to validate with.",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Print information about the processing.",
    )
    parser.add_argument(
        "--preload",
        "-p",
        action="store_true",
        help="Preload the data, rather than streaming from disk.",
    )
    parser.add_argument(
        "--from-scratch",
        "-f",
        action="store_true",
        help="Train from scratch, don't load an existing model.",
    )
    parser.add_argument(
        "--use-onecycle",
        action="store_true",
        help="Use the '1cycle' training scheduler.",
    )
    parser.add_argument(
        "--batch-size",
        "-b",
        type=int,
        default=1,
        help="Batch size.",
    )
    parser.add_argument(
        "--accumulate-grad-batches",
        "--acc-grad",
        type=int,
        default=None,
        help="Number of batches to accumulate gradient for.",
    )
    parser.add_argument(
        "--steps",
        "-s",
        type=int,
        default=600,
        help="Minimum training steps.",
    )
    parser.add_argument(
        "--learning-rate",
        "--lr",
        type=float,
        default=0.075,
        help="Learning rate.",
    )
    return parser


def main():
    parser = setup_parser()
    args = parser.parse_args()

    enable_wandb = False

    experiment_name = (
        "dense_1cyclelr_600epochs_075lr_defaultdiv_defaultanneal_blur64"
    )

    # Import this as lazily as possible as it takes a while to import, so that
    # we only pay the import cost when we use it.
    import torch

    melroot = mel.lib.fs.find_melroot()
    model_dir = melroot / mel.lib.fs.DEFAULT_CLASSIFIER_PATH
    model_path = model_dir / "automark4.pth"
    metadata_path = model_dir / "automark4.json"
    print(f"Will save to {model_path}")
    print(f"         and {metadata_path}")

    min_steps = args.steps
    total_steps = 1 + max((len(args.train) // args.batch_size), min_steps)

    model = mel.rotomap.detectmolesnn.CackModel4(
        total_steps=total_steps, use_onecycle=args.use_onecycle
    )

    if not args.from_scratch and model_path.exists():
        model.load_state_dict(torch.load(model_path))
        print(f"Loaded {model_path}.")

    model.learning_rate = args.learning_rate

    if enable_wandb:
        wandb_logger = pl.loggers.WandbLogger(
            project="mel-automark4", name=experiment_name
        )
        wandb_logger.watch(model, log="all")

    valid_dl = None
    if args.preload:
        data = [
            mel.rotomap.detectmolesnn.load_ptgz(example)
            for example in args.train
        ]
        train_dl = torch.utils.data.DataLoader(
            data,
            batch_size=args.batch_size,
            shuffle=True,
        )
        if args.valid:
            data = [
                mel.rotomap.detectmolesnn.load_ptgz(example)
                for example in args.valid
            ]
            valid_dl = torch.utils.data.DataLoader(
                data,
                batch_size=args.batch_size,
                shuffle=False,
            )
    else:
        train_dl = torch.utils.data.DataLoader(
            list(args.train),
            batch_size=args.batch_size,
            shuffle=True,
            collate_fn=mel.rotomap.detectmolesnn.collate_xym,
        )
        if args.valid:
            valid_dl = torch.utils.data.DataLoader(
                list(args.valid),
                batch_size=args.batch_size,
                shuffle=False,
                collate_fn=mel.rotomap.detectmolesnn.collate_xym,
            )

    trainer_kwargs = {
        "max_steps": model.total_steps,
        "log_every_n_steps": 1,
        "callbacks": [mel.rotomap.detectmolesnn.GlobalProgressBar()],
        "enable_checkpointing": False,
        "accelerator": "auto",
        "accumulate_grad_batches": args.accumulate_grad_batches,
        "val_check_interval": 5,
        # "auto_lr_find": True,
    }
    if enable_wandb:
        trainer_kwargs["logger"] = wandb_logger
    trainer = pl.Trainer(**trainer_kwargs)

    metadata = {}

    # print("Pre-tune learning rate:", model.learning_rate)
    # trainer.tune(model, train_dl)
    # print("Post-tune learning rate:", model.learning_rate)

    trainer.fit(model, train_dl, valid_dl)

    torch.save(model.state_dict(), model_path)

    if enable_wandb:
        wandb.finish()

    print(f"Saved {model_path}")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f)
        print(f"Saved {metadata_path}")

    pprint(trainer.logged_metrics)


if __name__ == "__main__":
    sys.exit(main())
