#! /usr/bin/env python
"""Train 'automark3'."""

import argparse
import json
import sys

import pytorch_lightning as pl
import wandb


import mel.rotomap.moles
import mel.lib.common
import mel.rotomap.detectmolesnn


def setup_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--train",
        metavar="PTGZ_FILE",
        nargs="+",
        help="A list of paths to pre-trained images to process.",
        required=True,
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Print information about the processing.",
    )
    parser.add_argument(
        "--preload",
        "-p",
        action="store_true",
        help="Preload the data, rather than streaming from disk.",
    )
    return parser


def main():
    parser = setup_parser()
    args = parser.parse_args()

    enable_wandb = False

    experiment_name = (
        "dense_1cyclelr_600epochs_075lr_defaultdiv_defaultanneal_blur64"
    )

    # Import this as lazily as possible as it takes a while to import, so that
    # we only pay the import cost when we use it.
    import torch

    melroot = mel.lib.fs.find_melroot()
    model_dir = melroot / mel.lib.fs.DEFAULT_CLASSIFIER_PATH
    model_path = model_dir / "automark3.pth"
    metadata_path = model_dir / "automark3.json"
    print(f"Will save to {model_path}")
    print(f"         and {metadata_path}")

    min_steps = 600
    batch_size = 4
    total_steps = 1 + max((len(args.train) // batch_size), min_steps)

    model = mel.rotomap.detectmolesnn.CackModel2(total_steps=total_steps)

    if enable_wandb:
        wandb_logger = pl.loggers.WandbLogger(
            project="mel-automark3", name=experiment_name
        )
        wandb_logger.watch(model, log="all")

    if args.preload:
        data = [
            mel.rotomap.detectmolesnn.load_ptgz(example)
            for example in args.train
        ]
        train_dl = torch.utils.data.DataLoader(
            data,
            batch_size=batch_size,
            shuffle=True,
        )
    else:
        assert False

    trainer_kwargs = {
        "max_steps": model.total_steps,
        "log_every_n_steps": 1,
        "callbacks": [mel.rotomap.detectmolesnn.GlobalProgressBar()],
        "enable_checkpointing": False,
        "accelerator": "auto",
        # "auto_lr_find": True,
    }
    if enable_wandb:
        trainer_kwargs["logger"] = wandb_logger
    trainer = pl.Trainer(**trainer_kwargs)

    metadata = {}

    # print("Pre-tune learning rate:", model.learning_rate)
    # trainer.tune(model, train_dl)
    # print("Post-tune learning rate:", model.learning_rate)

    trainer.fit(model, train_dl)

    torch.save(model.state_dict(), model_path)

    if enable_wandb:
        wandb.finish()

    print(f"Saved {model_path}")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f)
        print(f"Saved {metadata_path}")


if __name__ == "__main__":
    sys.exit(main())
